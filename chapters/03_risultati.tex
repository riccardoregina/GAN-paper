\section{Risultati}
Abbiamo implementato il framework descritto con \texttt{PyTorch} su dataset \texttt{MNIST}. Il codice è disponibile presso \url{https://github.com/TheFabbest/GAN}. Prima di analizzare i risultati, menzioniamo le modifiche apportate all'esperimento condotto in \cite{goodfellow2014generativeadversarialnetworks}.
Poiché \ref{alg:algorithm1} lo permetteva, abbiamo rimpiazzato \textit{SGD} con il più moderno \textit{ADAM}, con parametri $\beta_{1}=0.5$ e $\beta_{2}=0.999$.
Abbiamo sostituito \textit{MaxOut} con \textit{LeakyReLU} e applicato \textit{batch normalization} su $G$ per spingere i suoi neuroni ad apprendere pattern statisticamente indipendenti tra loro. La \textit{batch normalization} ha migliorato anche la stabilità del training.

Fissati degli iperparametri, durante il training abbiamo osservato tre possibili casi, di cui uno solo garantisce un buon apprendimento per $G$:
\begin{enumerate}
\item $G$ domina su $D$. In questo caso, $D$ non riesce a punire $G$ su pattern non propri ai dati reali, portando $G$ a non imparare bene. Questa situazione non perdura per tutto il training: arriverà un momento in cui $D$ avrà raggiunto una capacità tale da spronare $G$. Tuttavia, a quel punto, $G$ non riuscirà a correggere nel modo sperato i pattern appresi fino a quel momento.
\item $D$ domina su $G$. In questo caso, la troppa capacità discriminatoria di $D$ porta $G$ ad avere gradiente nullo e quindi a non imparare. Questa situazione, se verificatasi dall'inizio del training, porta $G$ a produrre pattern impropri ed estremamente rumorosi.
\item $D$ e $G$ hanno capacità confrontabile. In questo caso, il training avviene nel modo sperato. Tuttavia, affinché $G$ impari a generare realmente bene i dati, è necessario che questa situazione perduri per tutto il training. Questo è il motivo per cui trovare gli iperparametri risulta complesso.
\end{enumerate}

Abbiamo condotto il training al variare di alcuni iperparametri. Ne riassumeremo adesso le influenze sulla qualità della generazione finale: 
\begin{itemize}
\item Abbiamo sperimentato tre valori per la dimensione del mini-batch: $64$, $128$ e $256$, ottenendo i risultati migliori, in termini di qualità di generazione, per $64$ su reti profonde e per $256$ per reti più superficiali.
\item Abbiamo osservato che il valore della funzione di errore del generatore non è un buon indice della qualità delle generazioni. Quindi, non potendo fare completo affidamento sul valore delle funzioni di errore, non abbiamo implementato l'early stopping, permettendo un massimo di 500 epoche. Dopo aver raggiunto il miglior modello per gli iperparametri fissati, il generatore tende a spostare sempre più noise $z$ alla cifra $1$. Questo fenomeno è noto come \textbf{mode collapse}. Possiamo motivare questo fenomeno dicendo che il generatore, non essendo obbligato a saper generare tutte le cifre, concentra i suoi sforzi su quelle cifre che gli permettono di ingannare il discriminatore.
\item Valori maggiori di $1$ per $k$, ovvero il numero di aggiornamenti di $D$ per aggiornamenti di $G$, hanno reso troppo più veloce l'apprendimento di $D$, portando $D$ a dominare su $G$. Sebbene si possa pensare che aumentare il \textit{learning rate} per $D$ sia equivalente ad aumentare il valore di $k$, ciò non è vero: $k$ aggiornamenti con passo $lr$ sono generalmente più precisi, in termini di discesa del gradiente, rispetto ad un singolo aggiornamento con passo $k\cdot lr$.
\item Abbiamo osservato che valori bassi di \textit{learning rate} garantiscono cautela nell'apprendimento di $G$ e pertanto una migliore generazione. Ciò è tipico nel training di una rete e le GAN non fanno eccezione.
\item Abbiamo sperimentato l'uso di \textit{dropout} nel discriminatore. Sebbene l'uso di \textit{dropout} sia tipicamente causa di una convergenza più lenta, è anche vero che esso permette alla rete una maggiore capacità. Abbiamo visto che l'uso di dropout, per la maggior parte delle configurazioni, ha portato il discriminatore ad essere troppo forte rispetto al generatore e quindi a dominare il gioco. 
\item Abbiamo ottenuto risultati migliori per discriminatori meno profondi rispetto ai rispettivi generatori. Crediamo che questo sia dovuto alla difficoltà maggiore del task di $G$ rispetto a quello di $D$.
\end{itemize}

\begin{figure}
\centering
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=0.9\textwidth]{images/digits_1.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=0.9\textwidth]{images/digits_2.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=0.9\textwidth]{images/digits_3.png}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
\includegraphics[width=0.9\textwidth]{images/digits_4.png}
\end{subfigure}
\caption{I dati in figura sono stati generati campionando noise da $p_{z}$ ed utilizzando il generatore migliore trovato durante gli esperimenti.}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=0.9\textwidth]{images/digits_morph_1.png}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=0.9\textwidth]{images/digits_morph_2.png}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=0.9\textwidth]{images/digits_morph_3.png}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\includegraphics[width=0.9\textwidth]{images/digits_morph_4.png}
\end{subfigure}
\caption{Ciascuna sequenza di immagini è stata ottenuta campionando due noise da $p_{z}$ ed effettuando un'interpolazione tra i due. Ciò mostra il percorso che porta da una cifra all'altra.}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=0.9\textwidth]{images/training-case-1.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=0.9\textwidth]{images/training-case-2.png}
\caption{}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\includegraphics[width=0.9\textwidth]{images/training-case-3.png}
\caption{}
\end{subfigure}
\caption{I tre casi in cui può trovarsi il gioco: (a) $G$ domina su $D$; (b) $D$ domina su $G$; (c) $D$ e $G$ hanno capacità confrontabile. Le figure mostrano anche il processo con cui abbiamo valutato l'apprendimento del generatore.}
\end{figure}
