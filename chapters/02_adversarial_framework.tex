\section{Adversarial framework}
Definiamo come $p_{data}$ la distribuzione che genera i dati $x$, $p_{z}$ la distribuzione che genera dei rumori $z \in \bb{R}$. Sia $G(z;\theta_{g})$ una rete neurale, che chiameremo \textbf{generatore}, che effettua un mapping tra lo spazio dei noise e lo spazio dei dati. In altre parole, $G$ genera, a partire da un vettore di noise $\mathbf{z}$, un $x$ appartenente allo spazio dei dati reali. Sia $D(x;\theta_{d})$ una rete neurale, che chiameremo \textbf{discriminatore}, che assegna ad un dato $x$ appartenente allo spazio dei dati il valore $D(x)$, che corrisponde alla probabilità che $x$ sia un dato reale. La funzione di probabilità stimata da $D$ è: $$\bb{P}(t|x)=D(x)^{t}\cdot(1-D(x))^{1-t}$$ dove $t=1$ quando $x$ è reale, mentre $t=0$ quando è stato generato da $G$.

Presi $\{x^{(1)},...,x^{(m)}\}$ dati reali e $\{G(z^{(1)}),...,G(z^{(m)})\}$ dati generati da $G$, l'obiettivo di $D$ è massimizzare la sua likelihood $\cal{L}(D)$, che è data dal prodotto dei suoi output su dati reali e dati generati, ovvero: $$\cal{L}(D)=\prod_{i=1}^{m}{D(x^{(i)})} \cdot \prod_{i=1}^{m}{(1-D(G(z^{(i)})))}$$
Analogamente, $D$ vuole massimizzare la sua log-likelihood $\Lambda(D)$ e ovviamente anche la media di quest'ultima, ovvero: $$\frac{1}{m}\sum_{i=1}^{m}{\log (D(x^{(i)}))} + \frac{1}{m}\sum_{i=1}^{m}{\log(1-D(G(z^{(i)})))}$$
Per $m$ che tende all'infinito, aderendo all'approccio frequentistico, le medie aritmetiche si trasformano in medie in probabilità.
Se è vero che $D$ vuole massimizzare questa quantità, è anche vero che $G$ vuole minimizzarla, ovvero $G$ vuole che le sue generazioni siano indistinguibili dai dati reali.
In altre parole, $D$ e $G$ fanno parte del seguente gioco minimax: $$\min_{G}\max_{D}V(D,G)=\bb{E}_{x\sim p_{data}}[\log D(x)] + \bb{E}_{z \sim p_{g}}[\log(1-D(G({z})))]$$
che giunge all'equilibrio quando $p_{g}=p_{data}$, ovvero quando si ha $D(x)=\frac{1}{2}$ su tutto lo spazio dei dati. In tal caso, si osserva $V(D,G)=-\log 4$. Si noti che se $D$ arriva a dominare il gioco - ovvero è troppo più forte di $G$ - si ha $D(G(z))=0$.

La procedura di training \ref{alg:algorithm1} mostra come l'allenamento debba avvenire in modo alternato tra $G$ e $D$. In particolare, ai fini di un training di successo, bisogna assicurarsi che il gioco sia sempre bilanciato ed in particolare che $D$ sia sempre poco più avanti di $G$. Inoltre, durante le prime epoche, poiché $G$ è ancora scarso, si osserva $\log(1-D(G(z)))\approx 0$ e quindi anche $\nabla_{\theta_{g}}\approx 0$ e dunque $G$ apprende poco. Per questo motivo, invece di minimizzare $\log(1-D(G(z)))$, si può scegliere di massimizzare $\log D(G(z))$, il cui gradiente è molto maggiore all'inizio.

\begin{algorithm}
	\caption{Procedura di training.}
\label{alg:algorithm1}
\begin{algorithmic}[1]
	\For{number of epochs}
		\For{$k$ steps}
			\State Sample a mini-batch of $m$ noise samples $\{z^{(1)},...,z^{(m)}\}$ from $p_{g}$
			\State Sample a mini-batch of $m$ samples $\{x^{(1)},...,x^{(m)}\}$ from $p_{data}$
			\State Update the discriminator by ascending its stochastic gradient: $$\nabla_{\theta_{d}} \frac{1}{m}\sum_{i=1}^{m}\left[\log D(x^{(i)}) + \log (1-D(G(z^{(i)})))\right]$$
		\EndFor
		\State Sample a mini-batch of $m$ noise samples $\{z^{(1)},...,z^{(m)}\}$ from $p_{g}$
		\State Update the generator by descending its stochastic gradient: $$\nabla_{\theta_{d}} \frac{1}{m}\sum_{i=1}^{m}\left[\log (1-D(G(z^{(i)})))\right]$$
	\EndFor
\end{algorithmic}
\end{algorithm}
